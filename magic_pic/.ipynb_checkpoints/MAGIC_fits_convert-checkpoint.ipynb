{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprocessing data from MAGIC/PIC base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We start with some code to access the table at http://vobs.magic.pic.es/fits/\n",
    "def get_table_from_url(url):\n",
    "    from bs4 import BeautifulSoup as BS\n",
    "    import urllib2\n",
    "    soup = BS(urllib2.urlopen(url).read(), 'lxml')\n",
    "    table = soup.find('table',{'class','mytable'})\n",
    "    return table\n",
    "\n",
    "url = 'http://vobs.magic.pic.es/fits/'\n",
    "table_pic_html = get_table_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(unicode(table_pic_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This table now is parsed to get the columns\n",
    "def get_table_fields(table):\n",
    "\n",
    "    def get_doi_url(url):\n",
    "        # The url of the reference (\"aurl\").\n",
    "        from bs4 import BeautifulSoup as BS\n",
    "        import urllib2\n",
    "        import re\n",
    "        _ads = 'adsabs'\n",
    "        _cds = 'cdsads'\n",
    "        _axv = 'arxiv'\n",
    "        if (_ads in url or _cds in url or _axv in url):\n",
    "            soup = BS(urllib2.urlopen (url))\n",
    "            trs = soup.findAll('tr')\n",
    "            filter(lambda x: 'doi' in x.get_text().lower(), trs)\n",
    "            tr = filter(lambda x: 'doi' in x.get_text().lower(), trs)[0]\n",
    "            doi = tr.get_text()\n",
    "            url = 'http://dx.doi.org/'\n",
    "            url += re.sub('DOI:','',re.sub('\\n','',doi))\n",
    "        return url\n",
    "\n",
    "    def process_row(row):\n",
    "        cells = row.findAll('td')\n",
    "        if len(cells)==5:\n",
    "            # Object source name(s) (can be more then one comma separated)\n",
    "            src = cells[0].find(text=True)\n",
    "            src = src.strip()\n",
    "            # Article reference (url), usually a ref to ads\n",
    "            art = cells[1].find('a',href=True)\n",
    "            aurl = art['href']\n",
    "            durl = get_doi_url(aurl)\n",
    "            # We skip year of publication (third column)\n",
    "            # as well as bibcode reference (fourth column)\n",
    "            #ref = cells[3].find(text=True).encode('utf8')\n",
    "            # FITS file link for downloading it in the near future\n",
    "            fits = cells[4]\n",
    "            file = fits.find('a',href=True)\n",
    "            try:\n",
    "                file = file['href']\n",
    "            except:\n",
    "                file = None\n",
    "            furl = url+file if file!=None else '_NULL_'\n",
    "            return (src,durl,file)\n",
    "        return None\n",
    "    \n",
    "    magic_table = {'SOURCE':[], 'DOI':[], 'FITS':[]}\n",
    "    for row in table.findAll('tr'):\n",
    "        vals = process_row(row)\n",
    "        if vals is not None:\n",
    "            src,durl,file = vals\n",
    "            magic_table['SOURCE'].append(src)\n",
    "            magic_table['DOI'].append(durl)\n",
    "            magic_table['FITS'].append(file)\n",
    "    return magic_table\n",
    "    \n",
    "table_pic_dict = get_table_fields(table_pic_html)\n",
    "\n",
    "del table_pic_html\n",
    "#tf = table_filtered\n",
    "#for i in range(len(tf['source'])):\n",
    "#    print(\"%s : %s : %s\"%(tf['source'][i],tf['doi'][i],tf['fits'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_describe(table,include='all'):\n",
    "    print table.describe(include=include)\n",
    "    print \"\\n-> Has Nil?\"\n",
    "    hows_nil = table.isnull().any()\n",
    "    print hows_nil\n",
    "    for c in hows_nil.index:\n",
    "        if not hows_nil[c]: continue\n",
    "        print \"\\n-> Indexes where column '{}' is null:\".format(c)\n",
    "        print table[table[c].isnull()].index.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table_pic = pd.DataFrame(table_pic_dict)\n",
    "\n",
    "del table_pic_dict\n",
    "print_describe(table_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we want to download the fits files\n",
    "def download_fits(url,out):\n",
    "    import wget\n",
    "    filename = wget.download(url,out=out)\n",
    "    return filename\n",
    "\n",
    "def create_dir(dir):\n",
    "    import os\n",
    "    if not os.path.exists(dir):\n",
    "        try:\n",
    "            os.mkdir(dir)\n",
    "        except:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def clean_dir(dir,ext=\"*\"):\n",
    "    import os\n",
    "    from glob import glob\n",
    "    create_dir(dir)\n",
    "    if os.path.isdir(dir):\n",
    "        files = glob(os.path.join(dir,ext))\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def read_md5sum_file(md5txt='md5sum.txt'):\n",
    "    md5_hashs,md5_files = [],[]\n",
    "    with open(md5txt,'r') as mdf:\n",
    "        for line in mdf.readline():\n",
    "            _h,_f = line.split()\n",
    "            md5_hashs.append(_h.strip())\n",
    "            md5_files.append(_f.strip())\n",
    "    md5 = dict(zip(md5_files,md5_hashs))\n",
    "    return md5\n",
    "\n",
    "def create_md5sum_file(files_list,md5txt='md5sum.txt',dir=None):\n",
    "    import hashlib\n",
    "    md5 = {}\n",
    "    for f in files_list:\n",
    "        fname = os.path.join(dir,f) if dir else f\n",
    "        h = None\n",
    "        with open(fname,'rb') as fp:\n",
    "            h = hashlib.md5(fp.read()).hexdigest()\n",
    "        md5.update({f:h})\n",
    "    fname = os.path.join(dir,md5txt) if dir else md5txt\n",
    "    with open(fname,'w') as fp:\n",
    "        for file,hash in md5.iteritems():\n",
    "            fp.write(\"%s    %s\\n\"%(hash,file))\n",
    "    return md5\n",
    "\n",
    "def read_dir_content(dir,ext='*.fits'):\n",
    "    from glob import glob\n",
    "    dir_files_list = glob(os.path.join(dir,ext))\n",
    "    return [ os.path.basename(f) for f in dir_files_list ]\n",
    "\n",
    "def is_exist_files(dir,files_list):\n",
    "    import os\n",
    "    # First we see if there is a file list (md5sum) to look for\n",
    "    def check_md5sum(files_list,md5_file='md5sum.txt'):\n",
    "        if os.path.isfile(md5_file):\n",
    "            md5 = read_md5sum_file(md5_file)\n",
    "            md5_files_list = md5.keys()\n",
    "            leng_inters = len(set(md5_files_list).intersection(files_list))\n",
    "            return leng_inters == len(files_list)\n",
    "        # If there is *no* md5-file, return *None*\n",
    "        return None\n",
    "    # Also, check if the files are actually there (inside the dir)\n",
    "    def check_glob(files_list,dir):\n",
    "        files_ext = '*.fits'\n",
    "        dir_files_list = read_dir_content(dir,files_ext)\n",
    "        leng_matches = sum(map(lambda v: v in dir_files_list, files_list))\n",
    "        return leng_matches == len(files_list)\n",
    "    \n",
    "    md5_file = 'md5sum.txt'\n",
    "    md5_check = check_md5sum(files_list,md5_file)\n",
    "    if md5_check in (True,False):\n",
    "        return md5_check\n",
    "    glob_check = check_glob(files_list,dir)\n",
    "    if glob_check:\n",
    "        create_md5sum_file(files_list,dir=dir)\n",
    "    return glob_check\n",
    "\n",
    "\n",
    "fits_download_dir = 'FITS_pic/'\n",
    "\n",
    "FORCE_CLEAN = False\n",
    "if FORCE_CLEAN:\n",
    "    clean_dir(fits_download_dir)\n",
    "else:\n",
    "    create_dir(fits_download_dir)\n",
    "\n",
    "import os\n",
    "assert os.path.isdir(fits_download_dir),\"FITS files dir do not exist!\"\n",
    "\n",
    "fits_files = table_pic.FITS.dropna().apply(os.path.basename)\n",
    "if is_exist_files(fits_download_dir,fits_files):\n",
    "    print(\"FITS files exist locally. Passing by download step..\")\n",
    "    fits_local = fits_files.apply(lambda f: os.path.join(fits_download_dir,f))\n",
    "else:\n",
    "    print(\"FITS files do not exist locally. Downloading them...\")\n",
    "    furls = url + table_pic.FITS.dropna()\n",
    "    fits_local = furls.apply(lambda f: download_fits(f,fits_download_dir))\n",
    "    md5s = create_md5sum_file(fits_files,dir=fits_download_dir)\n",
    "    del furls\n",
    "\n",
    "table_pic['FITS_local'] = fits_local\n",
    "\n",
    "del fits_local,fits_files\n",
    "\n",
    "print_describe(table_pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Header keywords\n",
    " Primarily, regarding particularly MAGIC data, we should take a look at their [FITS format for MAGIC data](http://vobs.magic.pic.es/fits/mfits/tdas/tdas-fits.pdf) document, chapter 4. There we'll find the following list of keywords:\n",
    "\n",
    "__FITS standard__, _highlighted by MAGIC_:\n",
    " * AUTHOR\n",
    " * DATE\n",
    " * DATE-OBS\n",
    " * EXTNAME\n",
    " * OBJECT\n",
    " * ORIGIN\n",
    " * TELESCOP\n",
    " \n",
    "__MAGIC__, _to describe extension content_:\n",
    " * CONFLEVE\n",
    " * EFFICIEN\n",
    " * EMAX\n",
    " * EMIN\n",
    " * PERIOD\n",
    " * PHIMAX\n",
    " * PHIMIN\n",
    " * REFPAPER\n",
    " * SIZEMIN\n",
    " * SIZEMAX\n",
    " * TOBS\n",
    " * ZMIN\n",
    " * ZMAX\n",
    " * TZERO\n",
    " * VERSION\n",
    "\n",
    "__MAGIC__, _SPECTRUM extension_:\n",
    " * ISINTEGR : 'F' means density flux; 'I' means integrated flux.\n",
    " \n",
    "On the other hand, there is the [FITS standard](http://heasarc.gsfc.nasa.gov/docs/fcg/standard_dict.html) and the [FITS commons](http://heasarc.gsfc.nasa.gov/docs/fcg/common_dict.html) set of keywords.\n",
    "\n",
    "\n",
    "```\n",
    "This data dictionary lists the 53 keywords currently defined in the\n",
    "FITS Standard:\n",
    "\n",
    "(blank)  CROTAn   EQUINOX  NAXISn   TBCOLn   TUNITn\n",
    "AUTHOR   CRPIXn   EXTEND   OBJECT   TDIMn    TZEROn\n",
    "BITPIX   CRVALn   EXTLEVEL OBSERVER TDISPn   XTENSION\n",
    "BLANK    CTYPEn   EXTNAME  ORIGIN   TELESCOP\n",
    "BLOCKED  DATAMAX  EXTVER   PCOUNT   TFIELDS\n",
    "BSCALE   DATAMIN  GCOUNT   PSCALn   TFORMn\n",
    "BUNIT    DATE     GROUPS   PTYPEn   THEAP\n",
    "BZERO    DATE-OBS HISTORY  PZEROn   TNULLn\n",
    "CDELTn   END      INSTRUME REFERENC TSCALn\n",
    "COMMENT  EPOCH    NAXIS    SIMPLE   TTYPEn\n",
    "```\n",
    "\n",
    "```\n",
    "     Dictionary of Commonly Used FITS Keywords\n",
    "\n",
    "This data dictionary contains FITS keywords that have been widely used\n",
    "within the astronomical community.  It is recommended that these\n",
    "keywords only be used as defined here.  These keywords may be grouped\n",
    "within the following 7 broad categories:\n",
    "\n",
    "1. Keywords that describe the data or the FITS file itself:\n",
    "\n",
    "    TITLE FILENAME FILETYPE ROOTNAME\n",
    "    PROGRAM CREATOR CONFIGUR\n",
    "    NEXTEND HDUNAME HDUVER HDULEVEL\n",
    "    TLMINn TLMAXn TDMINn TDMAXn TDBINn\n",
    "    TSORTKEY PROGRAM CREATOR CONFIGUR\n",
    "    HDUCLASS HDUDOC HDUVERS HDUCLASn\n",
    "\n",
    "2.  Keywords that describe the observation:\n",
    "\n",
    "    SUNANGLE MOONANGL\n",
    "    RA DEC RA_NOM DEC_NOM\n",
    "    RA_OBJ DEC_OBJ RA_PNT DEC_PNT PA_PNT\n",
    "    RA_SCX DEC_SCX RA_SCY DEC_SXY RA_SCZ DEC_SCZ\n",
    "    ORIENTAT AIRMASS LATITUDE\n",
    "    OBJNAME OBS_ID\n",
    "\n",
    "3.  Keywords that describe the instrument that took the data:\n",
    "\n",
    "    OBS_MODE DATAMODE\n",
    "    APERTURE DETNAM FILTER FILTERn GRATING GRATINGn\n",
    "    SATURATE\n",
    "\n",
    "4.  Keywords that give the date or duration of the observation:\n",
    "\n",
    "    TIME-OBS TIME-END DATE-END\n",
    "    EXPOSURE EXPTIME TELAPSE ELAPTIME ONTIME LIVETIME\n",
    "\n",
    "5.  Keywords that denote non-standard FITS keyword format conventions:\n",
    "\n",
    "    HIERARCH INHERIT CONTINUE\n",
    "\n",
    "6.  File checksum keywords:\n",
    "\n",
    "    CHECKSUM DATASUM CHECKVER\n",
    "\n",
    "7.  Hierarchical file grouping keywords:\n",
    "\n",
    "    GRPNAME GRPIDn GRPLCn\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Improvement\n",
    "\n",
    " * Given all those lists of keywords, I will just add `RA` and `DEC`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following keywords may not be seen in the Extended BinTableHDU header, but inside the PrimaryHDU' header.\n",
    "# That's because the MAGIC data handling system decided to not repeat unnecessarily such data.\n",
    "# See the 'General Keywords' section (4.1) of the document 'FITS Format for MAGIC data', version 0.2.\n",
    "#\n",
    "MAGIC_FITS_STANDARD_KEYWORDS = [\n",
    "    'AUTHOR',\n",
    "    'DATE',\n",
    "    'DATE-OBS',\n",
    "    'EXTNAME',\n",
    "    'OBJECT',\n",
    "    'ORIGIN',\n",
    "    'TELESCOP']\n",
    "MAGIC_FITS_EXTENSION_KEYWORDS = [\n",
    "    'CONFLEVE',\n",
    "    'EFFICIEN',\n",
    "    'EMAX',\n",
    "    'EMIN',\n",
    "    'PERIOD',\n",
    "    'PHIMAX',\n",
    "    'PHIMIN',\n",
    "    'REFPAPER',\n",
    "    'SIZEMIN',\n",
    "    'SIZEMAX',\n",
    "    'TOBS',\n",
    "    'ZMIN',\n",
    "    'ZMAX',\n",
    "    'TZERO',\n",
    "    'VERSION']\n",
    "# These keywords will be used to update the SPECTRUM extensions with *all* of them\n",
    "FITS_KEYWORDS = MAGIC_FITS_STANDARD_KEYWORDS + MAGIC_FITS_EXTENSION_KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we can process the fits files themselves.\n",
    "# He start noting that we want the SPECTRUM Data Unit(s)\n",
    "#  available (or not) in the fits files; discard the other DU.\n",
    "# Things we want to do:\n",
    "# - get the OBJECT name\n",
    "# - get the each object position\n",
    "# - get the observation date\n",
    "# - transform the data vectors (x) to frequency(Hz) and (y) to flux(erg/s/cm2)\n",
    "# Then we should follow the following workflow:\n",
    "# - open the fits file\n",
    "# - find the necessary data unit (SPECTRUM)\n",
    "# - open its header\n",
    "#  - get some keywords from the header\n",
    "# - open its data; data here are vectors\n",
    "#  - it can be from 2 to 4 vectors\n",
    "#   - energy\n",
    "#   - flux\n",
    "#   - Denergy\n",
    "#   - Dflux\n",
    "#  - convert the ?energy vectors to 'Hz' units\n",
    "#  - convert the ?flux vectors to 'erg/s/cm2' units\n",
    "\n",
    "# Here we just define the functions we'll need..\n",
    "\n",
    "def read_file(filename):\n",
    "    from astropy.io import fits\n",
    "    try:\n",
    "        hdulist = fits.open(filename)\n",
    "    except:\n",
    "        hdulist = None\n",
    "    return hdulist\n",
    "    \n",
    "def select_dataUnits(hdulist,du_name='SPECTRUM'):\n",
    "    dui = [ i for i,du in enumerate(hdulist) if du_name in du.name ]\n",
    "    dus = [ hdulist[ii] for ii in dui ]\n",
    "    _du = filter(lambda x:du_name in x.name, hdulist)\n",
    "    assert dus == _du\n",
    "    return (dus,dui)\n",
    "    \n",
    "def resolve_name(name):\n",
    "    from astropy.coordinates import get_icrs_coordinates as get_coords\n",
    "    try:\n",
    "        icrs = get_coords(name)\n",
    "        pos = (icrs.ra.value,icrs.dec.value)\n",
    "    except:\n",
    "        pos = None\n",
    "    return pos\n",
    "\n",
    "def transf_data(table):\n",
    "    from astropy import units\n",
    "    Unit = units.Unit\n",
    "    import numpy as np\n",
    "    units.set_enabled_equivalencies(units.spectral())\n",
    "    uEn = Unit('Hz')\n",
    "    uFn = Unit('erg s-1 cm-2')\n",
    "    uEc = Unit('TeV')\n",
    "    conv = {Unit('ph TeV s-1 cm-2') : lambda x,y: (x/Unit('ph')).to(uFn),\n",
    "            Unit('ph TeV-1 s-1 cm-2') : lambda x,y: ((y.to(uEc)**2)*(x/Unit('ph'))).to(uFn),\n",
    "            Unit('ph s-1 cm-2') : lambda x,y: None,\n",
    "            Unit('GeV') : lambda x: x.to(uEn, equivalencies=units.spectral())}\n",
    "\n",
    "    vE = table['energy']\n",
    "    uE = vE.unit\n",
    "    vEn = conv[uE](vE)\n",
    "\n",
    "    vF = table['flux']\n",
    "    uF = vF.unit\n",
    "    vFn = conv[uF](vF,vE)\n",
    "\n",
    "    if vFn is None:\n",
    "        print \"vFn is None!?!:\",str(uF)\n",
    "        return False\n",
    "    \n",
    "    table['energy'] = vEn\n",
    "    table['energy'].unit = vEn.unit\n",
    "    table['flux'] = vFn\n",
    "    table['flux'].unit = vFn.unit\n",
    "\n",
    "    if 'Denergy' in table.colnames:\n",
    "        vDE = table['Denergy']\n",
    "        uDE = vDE.unit\n",
    "        vDEn = conv[uDE](vDE)\n",
    "        table['Denergy'] = vDEn\n",
    "        table['Denergy'].unit = vDEn.unit\n",
    "    else:\n",
    "        uDE = Unit('')\n",
    "        nullval = -999\n",
    "        vDEn = np.asarray([nullval]*len(vE),dtype=int)\n",
    "        table['Denergy'] = vDEn\n",
    "        table['Denergy'].unit = uDE\n",
    "        table['Denergy'].null = nullval\n",
    "\n",
    "    if 'Dflux' in table.colnames:\n",
    "        vDF = table['Dflux']\n",
    "        uDF = vDF.unit\n",
    "        vDFn = conv[uDF](vDF,vE) # Notice we use the energy bin/value of the measurement.\n",
    "        table['Dflux'] = vDFn\n",
    "        table['Dflux'].unit = vDFn.unit\n",
    "    else:\n",
    "        uDF = Unit('')\n",
    "        nullval = -999\n",
    "        vDFn = np.asarray([nullval]*len(vE),dtype=int)\n",
    "        table['Dflux'] = vDFn\n",
    "        table['Denergy'].unit = uDF\n",
    "        table['Denergy'].null = nullval\n",
    "    return True\n",
    "\n",
    "def fix_dateobs(date):\n",
    "    try:\n",
    "        dt = str(date).split('-')\n",
    "        y = int(dt[0])\n",
    "    except:\n",
    "        return '1999-01-01'\n",
    "    try:\n",
    "        m = int(dt[1])\n",
    "    except:\n",
    "        m = 1\n",
    "    try:\n",
    "        d = int(dt[2])\n",
    "    except:\n",
    "        d = 1\n",
    "    return '{:4d}-{:02d}-{:02d}'.format(y,m,d)\n",
    "\n",
    "def merge_header_keywords(header_p,header_s):\n",
    "    # Extension's header has the highest priority; keywords there\n",
    "    # should not be overwritten. Relevant keywords are the ones in:\n",
    "    # 'FITS_KEYWORDS'\n",
    "    f_header = {'COMMENT':[]}\n",
    "    _kw = list(set(header_p.keys()).intersection(FITS_KEYWORDS))\n",
    "    for k in _kw:\n",
    "        f_header.update({k : header_p[k]})\n",
    "    if 'COMMENT' in header_p.keys():\n",
    "        f_header['COMMENT'].extend(header_p['COMMENT'])\n",
    "    _kw = list(set(header_s.keys()).intersection(FITS_KEYWORDS))\n",
    "    for k in _kw:\n",
    "        f_header.update({k : header_s[k]})\n",
    "    if 'COMMENT' in header_s.keys():\n",
    "        f_header['COMMENT'].extend(header_s['COMMENT'])\n",
    "    return f_header\n",
    "    \n",
    "def proc_fits_file(fn,source,doi):\n",
    "    out = []\n",
    "    print \"----------------------------------------------------------------------\"\n",
    "    print \"\\nProcessing file:\",fn,\n",
    "\n",
    "    hdulist = read_file(fn)\n",
    "    if hdulist is None:\n",
    "        print \" ..failed: to open.\" \n",
    "        return None\n",
    "    \n",
    "    spectra,spec_du_i = select_dataUnits(hdulist)\n",
    "    if not len(spec_du_i):\n",
    "        print \" ..failed: no SPECTRUM\" \n",
    "        return None\n",
    "    del spectra\n",
    "    \n",
    "    # First of all, let me see the contents of the first header\n",
    "    phdu_header = hdulist[0].header\n",
    "    #print('PrimaryHDU header:')\n",
    "    #for key,value in phdu_header.items():\n",
    "    #    print('{0} = {1}'.format(key, value))\n",
    "    #print(\"\")\n",
    "\n",
    "    for i,i_du in enumerate(spec_du_i):\n",
    "        \n",
    "        # Read the extension directly into an astropy.table\n",
    "        from astropy.table import Table\n",
    "        \n",
    "        table = Table.read(fn,hdu=i_du)\n",
    "        #print('Extension table metadata:')\n",
    "        #for key, value in table.meta.items():\n",
    "        #    print('{0} = {1}'.format(key, value))\n",
    "        #print(\"\")\n",
    "\n",
    "        spec = hdulist[i_du]\n",
    "        #print('Extension SPECTRUM header:')\n",
    "        #for key,value in spec.header.items():\n",
    "        #    print('{0} = {1}'.format(key, value))\n",
    "        #print(\"\")\n",
    "        \n",
    "        header_new = merge_header_keywords(phdu_header,table.meta)\n",
    "        #print('New header dictionary:')\n",
    "        #for key,value in header_new.items():\n",
    "        #    print('{0} = {1}'.format(key, value))\n",
    "        #print(\"\")\n",
    "        \n",
    "        if not 'OBJECT' in header_new.keys():\n",
    "            print(\"No 'OBJECT defined for this table. Passing by...\")\n",
    "            continue\n",
    "            \n",
    "        if ('SRCPOS1' in table.meta.keys()) and ('SRCPOS2' in table.meta.keys()):\n",
    "            pos = table.meta['SRCPOS1'],table.meta['SRCPOS2']\n",
    "        elif ('SRCPOS1' in phdu_header.keys()) and ('SRCPOS2' in phdu_header.keys()):\n",
    "            pos = phdu_header['SRCPOS1'],phdu_header['SRCPOS2']\n",
    "        else:\n",
    "            pos = resolve_name(header_new['OBJECT'])\n",
    "        if pos is None:\n",
    "            pos = resolve_name(source)\n",
    "            if pos is None:\n",
    "                pos = (None,None)\n",
    "            else:\n",
    "                header_new['OBJECT'] = source\n",
    "                print '*** NOP, No good either! Using PIC source.',source\n",
    "\n",
    "        try:\n",
    "            header_new['DATE-OBS'] = fix_dateobs(header_new['DATE-OBS'])\n",
    "        except:\n",
    "            header_new['DATE-OBS'] = fix_dateobs(None)\n",
    "            \n",
    "        header_new['REFDOI'] = doi\n",
    "        header_new['RA'],header_new['DEC'] = pos\n",
    "        #for key,value in header_new.items():\n",
    "        #    print('{0} = {1}'.format(key, value))\n",
    "        #print(\"\")\n",
    "        \n",
    "        table.meta.update(header_new)\n",
    "    \n",
    "        ret = transf_data(table)\n",
    "        if ret is not True:\n",
    "            continue\n",
    "        out.append(table.copy())\n",
    "    return out\n",
    "\n",
    "#for i in range(len(table_pic)):\n",
    "#    specs_list = proc_fits_file(table_pic.FITS_local[i],table_pic.SOURCE[i])\n",
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_pic['SPECTRUM'] = table_pic.apply(lambda x: proc_fits_file(x.FITS_local,x.SOURCE,x.DOI), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_degeneracy(group):\n",
    "    row = group.irow(0)\n",
    "    columns = row.to_dict()\n",
    "    tdf = {}\n",
    "    tabs = row['SPECTRUM']\n",
    "    tdf['SPECTRUM'] = tabs\n",
    "    del columns['SPECTRUM']\n",
    "    tdf['OBJECT'] = [ t.meta['OBJECT'] for t in tabs ]\n",
    "    tdf['RA'] = [ t.meta['RA'] for t in tabs ]\n",
    "    tdf['DEC'] = [ t.meta['DEC'] for t in tabs ]\n",
    "    tdf['DATE-OBS'] = [ t.meta['DATE-OBS'] for t in tabs ]\n",
    "    for c in columns: tdf[c] = [row[c]]*len(tabs)\n",
    "    return pd.DataFrame(tdf)\n",
    "\n",
    "table_proc = table_pic.dropna().groupby('DOI',group_keys=False).apply(fix_degeneracy).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_table(table,filename,odir):\n",
    "    import os,re\n",
    "    filename = os.path.basename(filename)\n",
    "    filename = re.sub('[+]','p',filename)\n",
    "    extname = '_' + table.meta['EXTNAME'] + '.fits'\n",
    "    fitsfilename = os.path.join(odir,re.sub('.fits',extname,filename))\n",
    "    table.write(fitsfilename,format='fits')\n",
    "    votfilename = fitsfilename[:-5] + '.vot'\n",
    "    table.write(votfilename,format='votable')\n",
    "    return fitsfilename\n",
    "\n",
    "outdir = 'FITS_out/'\n",
    "clean_dir(outdir)\n",
    "\n",
    "table_proc['FITS'] = table_proc.apply(lambda d:write_table(d.SPECTRUM,d.FITS_local,outdir),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',200)\n",
    "pd.set_option('display.max_columns',10)\n",
    "pd.set_option('display.width',100)\n",
    "#del table_proc['PROC']\n",
    "#del table_proc['SPECTRUM']\n",
    "\n",
    "#print table_proc.describe(include='all')\n",
    "print table_proc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_describe(table_proc,include=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_final = table_proc.dropna()[['OBJECT','RA','DEC','DOI','FITS','DATE-OBS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_describe(table_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_final.to_csv('table_final_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
